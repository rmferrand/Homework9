[
  {
    "objectID": "Modeling_new.html",
    "href": "Modeling_new.html",
    "title": "New Models - Regression Expansion",
    "section": "",
    "text": "Purpose: This home builds on the exercises listed in Homework 8. Homework 8 primarily consisted of data analysis and multiple linear regression models, including complex models with interaction. Homework 9 now uses multiple functions to run lasso, regression trees, random bagging, and random forests. The best model is deterined at the end and presented. Models are fit to the entire training set, evaluated on the test set using RMSE and MAE, and then the appropriate summaries are provided.\n\nSetup from Homework 8:\nI have used include=FALSE to take out all of the unnecessary coding bits. Below shows the test and training splits, as well as the MLR recipes. Notice how MLR recipe 3 has the lowest rmse and highest r-squared, so it will be used in analysis on this homework to compare to the other models.\n\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\n\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\nstep_date(date, features = \"dow\") |&gt;\nstep_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\nstep_rm(date, date_dow) |&gt;\nstep_dummy(seasons, holiday, day_type) |&gt;\nstep_normalize(all_numeric(), -bike_count)\n\n\nMLR_rec2 &lt;- MLR_rec1 |&gt;\nstep_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\nstarts_with(\"seasons\")*temp +\ntemp*rainfall)\n\n\nMLR_rec3 &lt;- MLR_rec2 |&gt;\nstep_poly(temp,\nwind_speed,\nvis,\ndew_point_temp,\nsolar_radiation,\nrainfall,\nsnowfall,\nhumidity,\ndegree = 2)\n\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec1) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\nMLR_CV_fit2 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec2) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\nMLR_CV_fit3 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec3) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n\n\nrbind(MLR_CV_fit1 |&gt; collect_metrics(),\nMLR_CV_fit2 |&gt; collect_metrics(),\nMLR_CV_fit3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4284.       10 165.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0151 Preprocessor1_Model1\n3 rmse    standard   3156.       10 267.     Preprocessor1_Model1\n4 rsq     standard      0.898    10   0.0176 Preprocessor1_Model1\n5 rmse    standard   3057.       10 210.     Preprocessor1_Model1\n6 rsq     standard      0.904    10   0.0142 Preprocessor1_Model1\n\n\n\nfinal_fit &lt;- workflow() |&gt;\nadd_recipe(MLR_rec3) |&gt;\nadd_model(MLR_spec) |&gt;\nlast_fit(bike_split)\nfinal_fit |&gt;\ncollect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3268.    Preprocessor1_Model1\n2 rsq     standard       0.896 Preprocessor1_Model1\n\n\n\nfinal_fit |&gt;\nextract_fit_parsnip() |&gt;\ntidy()\n\n# A tibble: 29 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                          22447.      1533.    14.6   6.85e-35\n 2 seasons_Spring                       -1923.       245.    -7.84  1.56e-13\n 3 seasons_Summer                        8083.       921.     8.78  3.56e-16\n 4 seasons_Winter                       -4193.       997.    -4.21  3.70e- 5\n 5 holiday_No.Holiday                     884.       191.     4.64  5.86e- 6\n 6 day_type_Weekend                     -1105.       169.    -6.54  3.79e-10\n 7 seasons_Spring_x_holiday_No.Holiday    -26.9      257.    -0.105 9.17e- 1\n 8 seasons_Summer_x_holiday_No.Holiday   -125.       239.    -0.522 6.02e- 1\n 9 seasons_Winter_x_holiday_No.Holiday   -323.       188.    -1.72  8.77e- 2\n10 seasons_Spring_x_temp                 1850.       480.     3.86  1.48e- 4\n# ℹ 19 more rows\n\n\n\n\nTask 1: MLR\nAssignment starts here!\nPreviously, we had decided that MLR 3 was the best model from last time due to the lowest RMSE and the highest $R^2$. Let’s go ahead and fit it to the entire training set and see how it predicts on the test set using MAE and RMSE\n\nRMSE and MAE MLR\n\nMLR_wkf3 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec3) |&gt;\nadd_model(MLR_spec)\n\nMLR_wkf3 |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae)) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3268. Preprocessor1_Model1\n2 mae     standard       2075. Preprocessor1_Model1\n\n\n\n\nFinal Coefficients MLR\n\nMLR_wkf3 |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae)) |&gt;\n  extract_fit_parsnip() |&gt; tidy()\n\n# A tibble: 29 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                          22447.      1533.    14.6   6.85e-35\n 2 seasons_Spring                       -1923.       245.    -7.84  1.56e-13\n 3 seasons_Summer                        8083.       921.     8.78  3.56e-16\n 4 seasons_Winter                       -4193.       997.    -4.21  3.70e- 5\n 5 holiday_No.Holiday                     884.       191.     4.64  5.86e- 6\n 6 day_type_Weekend                     -1105.       169.    -6.54  3.79e-10\n 7 seasons_Spring_x_holiday_No.Holiday    -26.9      257.    -0.105 9.17e- 1\n 8 seasons_Summer_x_holiday_No.Holiday   -125.       239.    -0.522 6.02e- 1\n 9 seasons_Winter_x_holiday_No.Holiday   -323.       188.    -1.72  8.77e- 2\n10 seasons_Spring_x_temp                 1850.       480.     3.86  1.48e- 4\n# ℹ 19 more rows\n\n\nPretty solid! Let’s see how it will compare to future models now.\n\n\n\nTask 2: Lasso\npreviously we decided that model 3 was the best one. We will be using that as a baseline for comparison.\nLet’s make sure we are standardizing our predictors appropriately.\n\nLASSO_recipe &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_zv(all_numeric(), -all_outcomes()) |&gt;\n  step_normalize(all_numeric(), -all_outcomes())\n\nUsing tune and the recipes, we go ahead and create the model instance and workflow.\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_recipe) |&gt;\n  add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWe can now tune the grid and find the best model. Let’s graph this as well so that we can see what the penalty term looks like.\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200)) \n\nLASSO_grid[1, \".metrics\"][[1]]\n\n[[1]]\n# A tibble: 400 × 5\n    penalty .metric .estimator .estimate .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard       4784. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard       4784. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard       4784. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard       4784. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard       4784. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard       4784. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard       4784. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard       4784. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard       4784. Preprocessor1_Model009\n10 2.83e-10 rmse    standard       4784. Preprocessor1_Model010\n# ℹ 390 more rows\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\nSo this was the code provided. For some reason, the RMSE doesn’t change at all with this penalty. It may not be appropriate! So after a lot of digging and figuring things out (including deleting all of my code and copying down hw8 word for word, I have realized the penalty() is not large enough for this dataset in particular). Let’s try something new:\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n    tune_grid(\n        resamples = bike_10_fold,\n        grid = grid_regular(penalty(range = c(-5, 2), trans = log10_trans()), levels = 200)\n    )\n\nLASSO_grid[1, \".metrics\"][[1]]\n\n[[1]]\n# A tibble: 400 × 5\n     penalty .metric .estimator .estimate .config               \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                 \n 1 0.00001   rmse    standard       4784. Preprocessor1_Model001\n 2 0.0000108 rmse    standard       4784. Preprocessor1_Model002\n 3 0.0000118 rmse    standard       4784. Preprocessor1_Model003\n 4 0.0000128 rmse    standard       4784. Preprocessor1_Model004\n 5 0.0000138 rmse    standard       4784. Preprocessor1_Model005\n 6 0.0000150 rmse    standard       4784. Preprocessor1_Model006\n 7 0.0000163 rmse    standard       4784. Preprocessor1_Model007\n 8 0.0000176 rmse    standard       4784. Preprocessor1_Model008\n 9 0.0000191 rmse    standard       4784. Preprocessor1_Model009\n10 0.0000207 rmse    standard       4784. Preprocessor1_Model010\n# ℹ 390 more rows\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\nThere we go! Finally we are seeing a real curve go on here. Let’s now grab the lowest RMSE and finalize the workflow on it.\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1    61.5 Preprocessor1_Model194\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 61.509857885805\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nRMSE and MAE Lasso\nNow, we fit this to the entire training dataset and put it on the testing dataset to see how it performs:\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(bike_split,  metrics = metric_set(rmse, mae)) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4033. Preprocessor1_Model1\n2 mae     standard       3078. Preprocessor1_Model1\n\n\n\n\nFinal Coefficients Lasso\n\nLASSO_wkf |&gt;\n    finalize_workflow(lowest_rmse) |&gt;\n    last_fit(bike_split,  metrics = metric_set(rmse, mae)) |&gt; \n    extract_fit_parsnip() |&gt; \n    tidy()\n\n# A tibble: 14 × 3\n   term               estimate penalty\n   &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)          17446.    61.5\n 2 temp                  3055.    61.5\n 3 humidity                 0     61.5\n 4 wind_speed            -482.    61.5\n 5 vis                      0     61.5\n 6 dew_point_temp         409.    61.5\n 7 solar_radiation       3915.    61.5\n 8 rainfall             -1847.    61.5\n 9 snowfall              -302.    61.5\n10 seasons_Spring       -2323.    61.5\n11 seasons_Summer       -1258.    61.5\n12 seasons_Winter       -3641.    61.5\n13 holiday_No.Holiday     740.    61.5\n14 day_type_Weekend     -1006.    61.5\n\n\nMLR outperforms lasso on all fronts. Let’s move on to the regression tree. Ouch!\n\n\n\nTask 3: Regression Trees\nLet’s create the recipe for tree. Remember, no interaction! We will dummy and normalize the necessary variables.\n\ntree_rec &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  update_role(date, new_role = \"ID\") |&gt;\n  step_rm(date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_zv(all_numeric(), -all_outcomes()) |&gt;\n  step_normalize(all_numeric(), -all_outcomes())\n\nDefining the model complexity and engine allows us to then create a workflow object.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\nSet a temp variable, then we let tune_grid() work its magic to find tuning parameters using CV. We take the grid obtained and fit it with the workflow to examine metrics across folds.\n\ntemp &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_10_fold)\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1        2.15e- 9          2 rmse    standard   4772.       10 135.     Prepro…\n 2        2.15e- 9          2 rsq     standard      0.779    10   0.0175 Prepro…\n 3        7.65e- 3         15 rmse    standard   4125.       10 256.     Prepro…\n 4        7.65e- 3         15 rsq     standard      0.833    10   0.0238 Prepro…\n 5        1.54e- 4          4 rmse    standard   4384.       10 187.     Prepro…\n 6        1.54e- 4          4 rsq     standard      0.816    10   0.0199 Prepro…\n 7        3.21e- 2          6 rmse    standard   4772.       10 135.     Prepro…\n 8        3.21e- 2          6 rsq     standard      0.779    10   0.0175 Prepro…\n 9        2.29e- 6          7 rmse    standard   3881.       10 282.     Prepro…\n10        2.29e- 6          7 rsq     standard      0.850    10   0.0247 Prepro…\n11        9.88e- 9         13 rmse    standard   3837.       10 302.     Prepro…\n12        9.88e- 9         13 rsq     standard      0.850    10   0.0257 Prepro…\n13        7.55e- 8         10 rmse    standard   3843.       10 299.     Prepro…\n14        7.55e- 8         10 rsq     standard      0.849    10   0.0257 Prepro…\n15        6.94e-10         11 rmse    standard   3837.       10 302.     Prepro…\n16        6.94e-10         11 rsq     standard      0.850    10   0.0257 Prepro…\n17        8.69e- 4          5 rmse    standard   3987.       10 226.     Prepro…\n18        8.69e- 4          5 rsq     standard      0.843    10   0.0218 Prepro…\n19        3.98e- 6          8 rmse    standard   3867.       10 296.     Prepro…\n20        3.98e- 6          8 rsq     standard      0.850    10   0.0254 Prepro…\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid)\ntree_fits\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6432.       10 350.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.595    10   0.0484 Prepro…\n 3    0.000000001           1 rmse    standard   6432.       10 350.     Prepro…\n 4    0.000000001           1 rsq     standard      0.595    10   0.0484 Prepro…\n 5    0.00000001            1 rmse    standard   6432.       10 350.     Prepro…\n 6    0.00000001            1 rsq     standard      0.595    10   0.0484 Prepro…\n 7    0.0000001             1 rmse    standard   6432.       10 350.     Prepro…\n 8    0.0000001             1 rsq     standard      0.595    10   0.0484 Prepro…\n 9    0.000001              1 rmse    standard   6432.       10 350.     Prepro…\n10    0.000001              1 rsq     standard      0.595    10   0.0484 Prepro…\n# ℹ 90 more rows\n\n\nLet’s visualize this using multiple pipes and ggplot:\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  mutate(tree_depth = factor(tree_depth)) |&gt;\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis gives us a nice look at what the trees are doing. It appears a large amount of variables would be suitable for this model. Let’s get a deeper look:\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.001                11 rmse    standard   3817.    10    285. Preprocess…\n 2    0.001                15 rmse    standard   3817.    10    285. Preprocess…\n 3    0.0000000001         11 rmse    standard   3837.    10    302. Preprocess…\n 4    0.000000001          11 rmse    standard   3837.    10    302. Preprocess…\n 5    0.00000001           11 rmse    standard   3837.    10    302. Preprocess…\n 6    0.0000001            11 rmse    standard   3837.    10    302. Preprocess…\n 7    0.000001             11 rmse    standard   3837.    10    302. Preprocess…\n 8    0.00001              11 rmse    standard   3837.    10    302. Preprocess…\n 9    0.0001               11 rmse    standard   3837.    10    302. Preprocess…\n10    0.0000000001         15 rmse    standard   3837.    10    302. Preprocess…\n# ℹ 40 more rows\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"rmse\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1           0.001         11 Preprocessor1_Model38\n\n\nModel 38 appears to be the best, with the lowest RMSE. Let’s now take this and finalize it to the workflow, and do the last split on our bike data.\n\nRMSE and MAE Regression Trees\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3096. Preprocessor1_Model1\n2 mae     standard       2362. Preprocessor1_Model1\n\n\nRelatively low RMSE too! Let’s go ahead and extract the workflow and picture the final tree!\n\n\nPlot of Regression Tree Final Fit\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n   1) root 263 25650580000 17446.250  \n     2) temp&lt; -0.3426071 102  2047259000  7912.637  \n       4) seasons_Winter&gt;=0.561781 67   223015600  5365.940  \n         8) dew_point_temp&lt; -1.63283 18    18872060  3891.833 *\n         9) dew_point_temp&gt;=-1.63283 49   150661300  5907.449  \n          18) rainfall&gt;=-0.2746632 10    26433630  4323.600 *\n          19) rainfall&lt; -0.2746632 39    92709680  6313.564 *\n       5) seasons_Winter&lt; 0.561781 35   557870900 12787.740  \n        10) rainfall&gt;=-0.1821769 8    38088330  8332.500 *\n        11) rainfall&lt; -0.1821769 27   313939100 14107.810  \n          22) seasons_Spring&gt;=0.561781 14    74244170 11896.860 *\n          23) seasons_Spring&lt; 0.561781 13    97557220 16488.850 *\n     3) temp&gt;=-0.3426071 161  8459162000 23486.180  \n       6) solar_radiation&lt; -1.052491 16   321022200  8404.375 *\n       7) solar_radiation&gt;=-1.052491 145  4097181000 25150.380  \n        14) solar_radiation&lt; -0.5261992 12   128734600 18622.670 *\n        15) solar_radiation&gt;=-0.5261992 133  3410978000 25739.350  \n          30) temp&lt; 0.243362 38   711563400 22534.210  \n            60) vis&lt; -0.2893236 13   306237900 18987.850 *\n            61) vis&gt;=-0.2893236 25   156809800 24378.320  \n             122) seasons_Spring&gt;=0.561781 10    77514310 22941.900 *\n             123) seasons_Spring&lt; 0.561781 15    44907160 25335.930 *\n          31) temp&gt;=0.243362 95  2152897000 27021.400  \n            62) temp&gt;=1.303428 26   203635800 22659.620  \n             124) temp&gt;=1.51019 16    95537520 21331.440 *\n             125) temp&lt; 1.51019 10    34713490 24784.700 *\n            63) temp&lt; 1.303428 69  1268216000 28664.970  \n             126) temp&lt; 0.5814277 20   447327100 25375.700  \n               252) temp&gt;=0.43069 8   248977000 22874.620 *\n               253) temp&lt; 0.43069 12   114945100 27043.080 *\n             127) temp&gt;=0.5814277 49   516181500 30007.530  \n               254) dew_point_temp&gt;=0.9380604 15   136370200 27579.530 *\n               255) dew_point_temp&lt; 0.9380604 34   252371500 31078.710  \n                 510) wind_speed&lt; -0.5125848 8    62488340 29050.250 *\n                 511) wind_speed&gt;=-0.5125848 26   146837800 31702.850  \n                  1022) seasons_Summer&lt; 0.5391721 9    21511830 30134.330 *\n                  1023) seasons_Summer&gt;=0.5391721 17    91461580 32533.240 *\n\ntree_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\nTask 4: Random Bagging\nBagging starts off in a similar fashion to the regression tree. Though, notice how we have the agency to select if we use either regression or classification. Let’s go ahead and set up the workflow.\n\nbag_recipe &lt;- tree_rec\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(bag_recipe) |&gt;\n add_model(bag_spec)\n\nWe now can tune the grid and use the rmse and mae as the metrics of comparison. This allows us to get a group of models.\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = bike_10_fold,\n grid = grid_regular(cost_complexity(),\n levels = 15),\n metrics = metric_set(rmse, mae))\nbag_fit\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [30 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\nbag_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"rmse\") |&gt;\n arrange(mean)\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.93e- 9 rmse    standard   3261.    10    140. Preprocessor1_Model03\n 2        7.20e- 7 rmse    standard   3320.    10    175. Preprocessor1_Model07\n 3        1.39e- 5 rmse    standard   3327.    10    205. Preprocessor1_Model09\n 4        3.73e- 8 rmse    standard   3331.    10    185. Preprocessor1_Model05\n 5        8.48e- 9 rmse    standard   3333.    10    221. Preprocessor1_Model04\n 6        1.18e- 3 rmse    standard   3344.    10    190. Preprocessor1_Model12\n 7        1.64e- 7 rmse    standard   3353.    10    183. Preprocessor1_Model06\n 8        4.39e-10 rmse    standard   3356.    10    172. Preprocessor1_Model02\n 9        1   e-10 rmse    standard   3359.    10    185. Preprocessor1_Model01\n10        6.11e- 5 rmse    standard   3359.    10    173. Preprocessor1_Model10\n11        3.16e- 6 rmse    standard   3409.    10    187. Preprocessor1_Model08\n12        2.68e- 4 rmse    standard   3461.    10    194. Preprocessor1_Model11\n13        5.18e- 3 rmse    standard   3566.    10    227. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3891.    10    160. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4877.    10    154. Preprocessor1_Model15\n\n\n\nRSME and MAE of Bagging\nNow, we can use the select_best() function and also compare the RMSE and MAE for these:\n\nbag_best_params &lt;- select_best(bag_fit, metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1   0.00000000193 Preprocessor1_Model03\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(bike_split, metrics = metric_set(rmse, mae))\n\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2860. Preprocessor1_Model1\n2 mae     standard       2266. Preprocessor1_Model1\n\n\n\n\nBagging Variable Importance Plot\nOnce again, MAE and RMSE looking relatively low. We appear to have some fierce competition. Let’s produce a variable importance plot:\n\nbag_full_fit &lt;- bag_final_wkf |&gt;\n fit(bike_data)\n\nbag_final_model &lt;- extract_fit_engine(bag_full_fit)\n\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\nTemperature appears to be a super important variable, which is what we were seeing in linear regression as well.\n\n\n\nTask 5: Random Forests\nRandom forests follow the same idea of bagging for the most part. Note we set an importance of impurity such that we can use the importance() function later for the importance plot.\nLet’s set up our workflow:\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(bag_recipe) |&gt;\n add_model(rf_spec)\n\nNow, we can fit an tune on the training set using the appropriate metrics:\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = bike_10_fold,\n grid = 7,\n metrics = metric_set(rmse, mae))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nUsing RSME, we find the best parameters.\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"rmse\") |&gt;\n arrange(mean)\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    12 rmse    standard   3014.    10    202. Preprocessor1_Model2\n2    11 rmse    standard   3026.    10    200. Preprocessor1_Model3\n3     9 rmse    standard   3037.    10    204. Preprocessor1_Model1\n4     8 rmse    standard   3043.    10    196. Preprocessor1_Model7\n5     5 rmse    standard   3123.    10    194. Preprocessor1_Model6\n6     4 rmse    standard   3142.    10    181. Preprocessor1_Model4\n7     2 rmse    standard   3458.    10    171. Preprocessor1_Model5\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"rmse\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    12 Preprocessor1_Model2\n\n\n\nRSME and MAE of Random Forests\nLastly, and probably most importantly, we collect the metrics and see what the RMSE and MAE are looking like.\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(bike_split, metrics = metric_set(rmse, mae))\n\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2638. Preprocessor1_Model1\n2 mae     standard       2118. Preprocessor1_Model1\n\n\nThose look very low! We now have all of our model metrics, so we can compare everything at the end.\nVariable importance plot is done as follows:\n\n\nRF Variable Importance Plot\n\nrf_full_fit &lt;- rf_final_wkf |&gt;\n fit(bike_data)\n\nrf_final_model &lt;- extract_fit_engine(rf_full_fit)\n\nrf_imp &lt;- tibble(term = names(importance(rf_final_model)),value = importance(rf_final_model)) |&gt; arrange(desc(value))\nrf_imp\n\n# A tibble: 13 × 2\n   term                      value\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 temp               21390254872.\n 2 solar_radiation     6979268317.\n 3 seasons_Winter      2247214520.\n 4 rainfall            1250209659.\n 5 dew_point_temp       592540426.\n 6 vis                  459219572.\n 7 humidity             437987244.\n 8 seasons_Spring       418568876.\n 9 wind_speed           369207720.\n10 day_type_Weekend     197262974.\n11 seasons_Summer        68231826.\n12 holiday_No.Holiday    61742642.\n13 snowfall              26506286.\n\nrf_imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\nTask 6: Best Model and fit to Full Data\n\nbind_rows(\n  MLR_wkf3 |&gt;\n    last_fit(bike_split, metrics = metric_set(rmse, mae)) |&gt;\n    collect_metrics() |&gt;\n    select(.metric, .estimate) |&gt;\n    spread(.metric, .estimate) |&gt;\n    mutate(Model = \"MLR\"),\n  LASSO_wkf |&gt;\n    finalize_workflow(lowest_rmse) |&gt;\n    last_fit(bike_split, metrics = metric_set(rmse, mae)) |&gt;\n    collect_metrics() |&gt;\n    select(.metric, .estimate) |&gt;\n    spread(.metric, .estimate) |&gt;\n    mutate(Model = \"LASSO\"),\n  tree_final_fit |&gt;\n    collect_metrics() |&gt;\n    select(.metric, .estimate) |&gt;\n    spread(.metric, .estimate) |&gt;\n    mutate(Model = \"Tree\"),\n  bag_final_fit |&gt;\n    collect_metrics() |&gt;\n    select(.metric, .estimate) |&gt;\n    spread(.metric, .estimate) |&gt;\n    mutate(Model = \"Bag\"),\n  rf_final_fit |&gt;\n    collect_metrics() |&gt;\n    select(.metric, .estimate) |&gt;\n    spread(.metric, .estimate) |&gt;\n    mutate(Model = \"RandomForest\")\n)\n\n# A tibble: 5 × 3\n    mae  rmse Model       \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 2075. 3268. MLR         \n2 3078. 4033. LASSO       \n3 2362. 3096. Tree        \n4 2266. 2860. Bag         \n5 2118. 2638. RandomForest\n\n\nFrom this, I would personally say the random forest model is the best! It has the lowest RMSE by a lot and a decently low MAE as well. Lasso is definitely the worst. MLR is second best!\n\nrf_best_model &lt;- rf_final_wkf |&gt;\n  fit(bike_data)\nrf_best_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~12L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             12 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7476327 \nR squared (OOB):                  0.9242882"
  }
]